{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8462adde-5b62-4603-a9a7-e922ce1cf814",
   "metadata": {},
   "source": [
    "# Building GPT2 from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba4be3-3005-4d11-8222-7bd5c8ef07a3",
   "metadata": {},
   "source": [
    "Source: \n",
    "* GPT2 min: https://github.com/The-Pocket/PocketFlow-Tutorial-Video-Generator/blob/main/docs/llm/transformer.md\n",
    "* KV caching: https://github.com/The-Pocket/PocketFlow-Tutorial-Video-Generator/blob/main/docs/llm/kv_cache.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c0a7b-94d1-41de-a899-3a0ddc944568",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c5d2a10-40c4-4805-9d78-d621c1a37cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from dataclasses import dataclass\n",
    "import importlib\n",
    "import shared_utilities\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "import os\n",
    "\n",
    "# Fix 2: Suppress tokenizer warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# This command forces Python to re-read the .py file from the disk\n",
    "importlib.reload(shared_utilities)\n",
    "\n",
    "from shared_utilities import HFTextDataModule, LightningModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d32ce9-377e-42df-a4d9-e602c5310c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int\n",
    "    block_size: int\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.1\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.n_head, self.n_embd = config.n_head, config.n_embd\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.resid_drop = nn.Dropout(config.dropout)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "    \n",
    "    def forward(self, x, past_kv=None):\n",
    "        B, T, C = x.size() # Note: T is the new sequence length, usually 1 during generation\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        head_dim = C // self.n_head\n",
    "        q = q.view(B, T, self.n_head, head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            k = torch.cat((past_k, k), dim=-2) # Concatenate along the sequence length dimension\n",
    "            v = torch.cat((past_v, v), dim=-2)\n",
    "\n",
    "        present_kv = (k, v)\n",
    "        T_total = k.size(-2)\n",
    "        # Perform the attention calculation\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(head_dim))\n",
    "        att = att.masked_fill(self.bias[:, :, T_total-T:T_total, :T_total] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.resid_drop(self.c_proj(y)), present_kv\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.proj = nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.proj(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(self.ln_1(x), past_kv=None)  # Ignore KV cache\n",
    "        x = x + attn_output\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.wte.weight\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0)\n",
    "        x = self.wte(idx) + self.wpe(pos)\n",
    "        x = self.drop(x)\n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) if targets is not None else None\n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens=50, temperature=1.0, top_k=None):\n",
    "        past_kv = None\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Only pass new token after first iteration\n",
    "            idx_cond = idx if past_kv is None else idx[:, -1:]\n",
    "            idx_cond = idx_cond[:, -self.config.block_size:]\n",
    "            \n",
    "            # Forward with cache\n",
    "            logits, past_kv = self.forward_with_cache(idx_cond, past_kv)\n",
    "            logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
    "            \n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('inf')\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "    def forward_with_cache(self, idx, past_kv=None):\n",
    "        B, T = idx.size()\n",
    "        \n",
    "        # Adjust position embeddings based on cache\n",
    "        pos_offset = 0 if past_kv is None else past_kv[0][0].shape[-2]\n",
    "        pos = torch.arange(pos_offset, pos_offset + T, device=idx.device)\n",
    "        \n",
    "        x = self.wte(idx) + self.wpe(pos)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        new_past_kv = []\n",
    "        for i, block in enumerate(self.h):\n",
    "            layer_past = past_kv[i] if past_kv else None\n",
    "            x = x + block.ln_1(x)\n",
    "            x, present = block.attn(x, layer_past)\n",
    "            new_past_kv.append(present)\n",
    "            x = x + block.mlp(block.ln_2(x))\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits, new_past_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a01e7dc-ccaf-4360-8145-5b08dcf0cc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n",
      "\n",
      "✅ Dataset loaded:\n",
      "   Vocab size: 50,257\n",
      "   Train tokens: 2,391,884\n",
      "   Val tokens: 247,289\n",
      "   Test tokens: 283,287\n",
      "   Train samples: 2,391,628\n",
      "   Val samples: 247,033\n"
     ]
    }
   ],
   "source": [
    "# Initialize data module\n",
    "dm = HFTextDataModule(\n",
    "    dataset_name=\"wikitext\",\n",
    "    dataset_config=\"wikitext-2-raw-v1\",\n",
    "    batch_size=24,\n",
    "    block_size=256,\n",
    "    num_workers=8,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "dm.prepare_data()\n",
    "# Setup tokenization and datasets\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a43b0de-0d6b-472d-9a04-7765c376fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INSPECTING FIRST 5 SAMPLES FROM WIKITEXT-2\n",
      "================================================================================\n",
      "\n",
      "Dataset Info:\n",
      "  Vocab size: 50,257\n",
      "  Train samples: 2,391,628\n",
      "  Block size: 256\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sample 1:\n",
      "  Input shape:  torch.Size([256])\n",
      "  Target shape: torch.Size([256])\n",
      "\n",
      "  Input text (first 50 tokens):\n",
      "     = Valkyria Chronicles III = \n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) ,\n",
      "\n",
      "  Target text (first 50 tokens, shifted by 1):\n",
      "     Valkyria Chronicles III = \n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 2:\n",
      "  Input shape:  torch.Size([256])\n",
      "  Target shape: torch.Size([256])\n",
      "\n",
      "  Input text (first 50 tokens):\n",
      "     Valkyria Chronicles III = \n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly\n",
      "\n",
      "  Target text (first 50 tokens, shifted by 1):\n",
      "    alkyria Chronicles III = \n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 3:\n",
      "  Input shape:  torch.Size([256])\n",
      "  Target shape: torch.Size([256])\n",
      "\n",
      "  Input text (first 50 tokens):\n",
      "    alkyria Chronicles III = \n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred\n",
      "\n",
      "  Target text (first 50 tokens, shifted by 1):\n",
      "    ria Chronicles III = \n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 4:\n",
      "  Input shape:  torch.Size([256])\n",
      "  Target shape: torch.Size([256])\n",
      "\n",
      "  Input text (first 50 tokens):\n",
      "    ria Chronicles III = \n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to\n",
      "\n",
      "  Target text (first 50 tokens, shifted by 1):\n",
      "     Chronicles III = \n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 5:\n",
      "  Input shape:  torch.Size([256])\n",
      "  Target shape: torch.Size([256])\n",
      "\n",
      "  Input text (first 50 tokens):\n",
      "     Chronicles III = \n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as\n",
      "\n",
      "  Target text (first 50 tokens, shifted by 1):\n",
      "     III = \n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as V\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"INSPECTING FIRST 5 SAMPLES FROM WIKITEXT-2\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"  Vocab size: {len(dm.tokenizer):,}\")\n",
    "print(f\"  Train samples: {len(dm.train):,}\")\n",
    "print(f\"  Block size: {dm.block_size}\")\n",
    "\n",
    "# Inspect first 5 samples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "for i in range(5):\n",
    "    inputs, targets = dm.train[i]\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Input shape:  {inputs.shape}\")\n",
    "    print(f\"  Target shape: {targets.shape}\")\n",
    "    \n",
    "    # ✅ Decode tokens to text (HuggingFace way)\n",
    "    input_text = dm.tokenizer.decode(inputs[:50])\n",
    "    target_text = dm.tokenizer.decode(targets[:50])\n",
    "    \n",
    "    print(f\"\\n  Input text (first 50 tokens):\")\n",
    "    print(f\"    {input_text}\")\n",
    "    \n",
    "    print(f\"\\n  Target text (first 50 tokens, shifted by 1):\")\n",
    "    print(f\"    {target_text}\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdddc03f-42b4-4032-8386-6fce822c91df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "L.pytorch.seed_everything(123)\n",
    "# Fix 1: Enable Tensor Cores (2x speedup)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# pytorch_model = GPT2(GPTConfig(vocab_size=len(dm.tokenizer), block_size=256))\n",
    "# Fix 3: Compile the model (1.5-2x speedup)\n",
    "config = GPTConfig(vocab_size=len(dm.tokenizer), block_size=256)\n",
    "pytorch_model = GPT2(config)\n",
    "# pytorch_model = torch.compile(pytorch_model, mode='reduce-overhead')  # Add this\n",
    "\n",
    "lightning_model = LightningModel(\n",
    "    model=pytorch_model,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=500\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"gpt2-{epoch:02d}-{val_loss:.2f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=3,\n",
    ")\n",
    "\n",
    "# Disabling it because can't be used withou Logger\n",
    "#lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=\"bf16-mixed\",        # Add this - 40% memory savings\n",
    "    accumulate_grad_batches=4,     # Add this - effective batch size 64\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=False, # skipping due to problems with CSVLogger(save_dir=\"logs/\", name=\"gpt2_training\"),\n",
    "    gradient_clip_val=1.0,\n",
    "    log_every_n_steps=50,\n",
    "    val_check_interval=0.25,  # Validate 4 times per epoch\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a1bdfb8-6a3b-4a66-b1e3-634993fafe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/borja-dosuna/miniconda3/envs/vit-env/lib/python3.13/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:242: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type | Params | Mode  | FLOPs\n",
      "-----------------------------------------------\n",
      "0 | model | GPT2 | 123 M  | train | 0    \n",
      "-----------------------------------------------\n",
      "123 M     Trainable params\n",
      "0         Non-trainable params\n",
      "123 M     Total params\n",
      "495.400   Total estimated model params size (MB)\n",
      "139       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Dataset loaded:\n",
      "   Vocab size: 50,257\n",
      "   Train tokens: 2,391,884\n",
      "   Val tokens: 247,289\n",
      "   Test tokens: 283,287\n",
      "   Train samples: 2,391,628\n",
      "   Val samples: 247,033\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e43655f44e47cabad4e18d451fc23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating val dataloader...\n",
      "Val dataloader created with 10294 batches\n",
      "Creating train dataloader...\n",
      "Train dataloader created with 99652 batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcaf34241294e2b853f0d9a503450c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7098d36e8d2d4fbbb9c62338554e2bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5952c9a302a04ec58bbe77c174c8e108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c99da79f7b843b78f176c5d3e2760a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd69bd88c0c4f38bfc6800f813d0315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=lightning_model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e65e531-efd8-4606-bcd0-6fc2648792df",
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "No `test_step()` method defined to run `Trainer.test`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMisconfigurationException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlightning_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vit-env/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:821\u001b[39m, in \u001b[36mTrainer.test\u001b[39m\u001b[34m(self, model, dataloaders, ckpt_path, verbose, datamodule, weights_only)\u001b[39m\n\u001b[32m    819\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    820\u001b[39m \u001b[38;5;28mself\u001b[39m.testing = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vit-env/lib/python3.13/site-packages/lightning/pytorch/trainer/call.py:49\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     52\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vit-env/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:864\u001b[39m, in \u001b[36mTrainer._test_impl\u001b[39m\u001b[34m(self, model, dataloaders, ckpt_path, verbose, datamodule, weights_only)\u001b[39m\n\u001b[32m    860\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    861\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    862\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn, ckpt_path, model_provided=model_provided, model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    863\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[32m    866\u001b[39m results = convert_tensors_to_scalars(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vit-env/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:1027\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path, weights_only)\u001b[39m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28mself\u001b[39m._callback_connector._attach_model_callbacks()\n\u001b[32m   1025\u001b[39m \u001b[38;5;28mself\u001b[39m._callback_connector._attach_model_logging_functions()\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m \u001b[43m_verify_loop_configurations\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1030\u001b[39m \u001b[38;5;66;03m# SET UP THE TRAINER\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1032\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: setting up strategy environment\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vit-env/lib/python3.13/site-packages/lightning/pytorch/trainer/configuration_validator.py:41\u001b[39m, in \u001b[36m_verify_loop_configurations\u001b[39m\u001b[34m(trainer)\u001b[39m\n\u001b[32m     39\u001b[39m     __verify_eval_loop_configuration(model, \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m trainer.state.fn == TrainerFn.TESTING:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[43m__verify_eval_loop_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m trainer.state.fn == TrainerFn.PREDICTING:\n\u001b[32m     43\u001b[39m     __verify_eval_loop_configuration(model, \u001b[33m\"\u001b[39m\u001b[33mpredict\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vit-env/lib/python3.13/site-packages/lightning/pytorch/trainer/configuration_validator.py:106\u001b[39m, in \u001b[36m__verify_eval_loop_configuration\u001b[39m\u001b[34m(model, stage)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_step:\n\u001b[32m    105\u001b[39m     trainer_method = \u001b[33m\"\u001b[39m\u001b[33mvalidate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stage == \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m stage\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m()` method defined to run `Trainer.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# check legacy hooks are not present\u001b[39;00m\n\u001b[32m    109\u001b[39m epoch_end_name = \u001b[33m\"\u001b[39m\u001b[33mvalidation_epoch_end\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stage == \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtest_epoch_end\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mMisconfigurationException\u001b[39m: No `test_step()` method defined to run `Trainer.test`."
     ]
    }
   ],
   "source": [
    "trainer.test(model=lightning_model, datamodule=dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
